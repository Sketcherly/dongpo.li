---
title: "6、文件服务器"
date: 2023-08-01T16:32:08+08:00
draft: false
summary: '存储是根本，先把存储搞定。  
思路是：一个单独的虚拟机(2C4G)作为数据存储服务器，直通两个机械硬盘(4T*2)作为存储，两个盘使用`mdadm`组Raid1，再部署一个NFS开放给其他机器使用，NFS也可以作为局域网内传文件和大文件上传服务器的手段。'
---


## 虚拟机
参见上一篇，不再详细说明了。


## 直通硬盘

在PVE的宿主机上执行以下命令
``` shell
# 查看硬盘的id
ls /dev/disk/by-id
qm set 100 -scsi1 /dev/disk/by-id/ata-WDC_WD42PURU-64C4CY0_WD-WXE2A23MPV30
qm set 100 -scsi2 /dev/disk/by-id/ata-WDC_WD42PURU-64C4CY0_WD-WXE2A23MPA7P
```

## 新硬盘组Raid1

注意：新盘要先分区，分区之后使用分区组Raid，裸盘组软Raid会有问题，原来这是常识。重启了N次才知道。

参见 https://www.v2ex.com/t/624554

以下是原文备份
```
家里的一台 Linux 主机，加了两块 1T 硬盘，最近打算组成 raid 用来下 PT，由于没有阵列卡主板也不支持 raid，于是就用 mdadm 软 raid，创建过程：

mdadm -Cv /dev/md0 -a yes -n 2 -l 0 /dev/sdb /dev/sdc
mkfs.xfs -f /dev/md0
一切正常，最后把挂载信息写入 /etc/fstab
结果重启就问题了，卡在进度条，几分钟后自动进入了救援模式（显示的什么内容忘记了）
经过多次测试，确定是重启之后，/dev/md0 设备就直接消失了。。。消失了。。。消失了。。。

查了很久的资料，尝试过修改 /etc/mdadm/mdadm.conf 文件，在里面增加 ARRAY /dev/md0......的方法，但是并没有效果
直到我看到了这个，这个问题应该是和 GPT 分区表有关：
https://unix.stackexchange.com/questions/156424/centos-7-created-mdadm-array-disappears-after-reboot

于是我先尝试了一个简单的方法，进 PE 把两块硬盘从 GPT 转成 MBR，然后重新创建 raid，重启后就没有消失了，但是这个简单的方法并不完美，因为 MBR 最大只支持 2TB 的硬盘
最后还是使用 GPT，但是在创建 raid 之前要使用 gdisk 命令在两块硬盘上各创建一个分区，/dev/sdb1 和 /dev/sdc1，然后再使用分区创建 raid：

mdadm -Cv /dev/md0 -a yes -n 2 -l 0 /dev/sdb1 /dev/sdc1
mkfs.xfs -f /dev/md0
这次重启之后 /dev/md0 没有消失，最后将挂载信息写入 /etc/fstab，一切正常


-------------
给后来者的提示：

这个问题主要原因是用于组阵列的硬盘没有正确清除原始的信息，导致磁盘被首先认作 GPT 磁盘而非 MDADM 成员。
解决方法也很简单，就是清除掉 GPT 分区表，即清除掉头部的数个扇区和尾部的数个扇区，就行了。
也可以先转换成 MBR （转换过程中会自动清除 GPT 分区表），然后再清除 MBR 分区表（第一个扇区）即可。
没有必要先分区再组阵列。



-------------
裸盘做 raid========骚操作?

一个合格的 raid 程序，给裸盘建 raid 时，就必须！！！清除头尾 n 个扇区，以免被误认。
然后在裸盘头尾写上自己家的数据。

很显然这个 mdadm 程序不合格，或者 centos 中的 mdadm 版本太老。



```


``` shell
# 使用盘符的方式，每次重启盘符可能会漂移，组Raid会失败，机器启动也会失败，这种方式废弃了。
# mdadm -Cv /dev/md0 -n 2 -l 1 /dev/sdb /dev/sdc

# 注意，这个命令可能会擦除硬盘上的所有数据，谨慎执行！！！
mdadm -Cv /dev/md0 -n 2 -l 1 /dev/disk/by-id/scsi-0QEMU_QEMU_HARDDISK_drive-scsi1-part1 /dev/disk/by-id/scsi-0QEMU_QEMU_HARDDISK_drive-scsi2-part1

# 查看raid1的信息
mdadm -D /dev/md0

# 将盘符数据写进配置文件，raid配置重启自动生效
mdadm --detail --scan >> /etc/mdadm/mdadm.conf

# 这个要特别注意，每次修改完/etc/mdadm/mdadm.conf之后都要执行一次，具体可以参见自动生成的配置文件/etc/mdadm/mdadm.conf中的注释说明
update-initramfs -u

# 重启测试挂载情况
reboot
```


```
cat /etc/mdadm/mdadm.conf
```

``` txt
# mdadm.conf
#
# !NB! Run update-initramfs -u after updating this file.
# !NB! This will ensure that initramfs has an uptodate copy.
#
# Please refer to mdadm.conf(5) for information about this file.
#

# by default (built-in), scan all partitions (/proc/partitions) and all
# containers for MD superblocks. alternatively, specify devices to scan, using
# wildcards if desired.
#DEVICE partitions containers

# automatically tag new arrays as belonging to the local system
HOMEHOST <system>

# instruct the monitoring daemon where to send mail alerts
MAILADDR root

# definitions of existing MD arrays

# This configuration was auto-generated on Mon, 01 Apr 2024 06:57:36 +0000 by mkconf
DEVICE /dev/disk/by-id/scsi-0QEMU_QEMU_HARDDISK_drive-scsi1-part1 /dev/disk/by-id/scsi-0QEMU_QEMU_HARDDISK_drive-scsi2-part1
# 这行在上边的步骤中已经自动生成了，主要是上边这一行，实测其实没有也行
ARRAY /dev/md0 metadata=1.2 name=StorageServer:0 UUID=748c69ff:d6bc60de:0899c43e:88dab0a3 
```





## 重装系统恢复Raid1
apt install mdadm


-- 先尝试这个
``` shell
mdadm --assemble /dev/md0 /dev/disk/by-id/scsi-0QEMU_QEMU_HARDDISK_drive-scsi1-part1 /dev/disk/by-id/scsi-0QEMU_QEMU_HARDDISK_drive-scsi2-part1
```

-- 不行的话再尝试这个（注意：这是重组Raid命令，不到万不得已不建议，虽然到目前为止没失败过）
``` shell
mdadm --create /dev/md0 --assume-clean --level=1 --verbose --raid-devices=2 /dev/disk/by-id/scsi-0QEMU_QEMU_HARDDISK_drive-scsi1-part1 /dev/disk/by-id/scsi-0QEMU_QEMU_HARDDISK_drive-scsi2-part1
```


``` shell
mdadm --detail --scan >> /etc/mdadm/mdadm.conf
update-initramfs -u
reboot
```

``` bash
vim /etc/fstab
```

```
/dev/md0 /data ext4 defaults 0 0
```


# 参考资料
https://www.dreamxu.com/debian-create-software-raid-one-arrays/


# 安装NFS服务
``` bash
apt install nfs-kernel-server
systemctl enable nfs-server
vim /etc/exports
```

```
/data 192.168.1.0/24(rw,sync,no_root_squash,no_subtree_check,insecure)
```

> insecure参数是专门给mac连加的

```
systemctl restart nfs-server
systemctl status nfs-server
```


## 客户端挂载NFS

``` bash
apt install nfs-common
vim /etc/fstab
```

```
192.168.1.201:/data /data nfs4 rw,nosuid 0 0
```

临时挂载命令
```
mount -t nfs4 192.168.1.201:/data /data
```